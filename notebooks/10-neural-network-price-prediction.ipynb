{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neuronales Netz zur Vorhersage von Fahrtkosten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fragestellung: Prognostizieren von Fahrtkosten: Wie teuer ist eine Fahrt, wenn ich in einer bestimmten Zone zu einer bestimmten Zeit einsteige?\n",
    "Als Datenbasis für das Training werden die Taxisdaten aus dem Jahr 2019 verwendet.\n",
    "In der Folge werden hierfür zwei Testfälle betrachtet. In dem einen ersten werden die Taxidaten aus dem Jahr 2019 aufgeteilt, sodass ein Teil der Daten als Testdaten verwendet werden.\n",
    "In dem zweiten Fall werden die Daten aus einem zukünftigen Monat verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Laden der Daten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zuerst werden die Daten aus der csv-Datei geladen. Hiefür wurde vorher seitens der Gruppe eine vorbearbeitete csv Datei verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_data = pd.read_csv(\"../data/processed/randomized_aggregated_for_one_year.csv\")\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing der Daten\n",
    "Im nächsten Schritt werden die eingelesenen Daten vorbearbeitet. Wichtig ist es, dass null Werte gegen 0 ersetzt werden, auch werden die Datumswerte in Integer-Werte verändert. Zudem werden sämtliche String-Datentypen aus dem Dataframe entfernt.\n",
    "Des Weiteren werden weitere Spalten angelegt, sodass die aktuelle Uhrzeit oder der aktuelle Monat mittels Integer wiedergegeben werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_data['tpep_pickup_datetime'] = pd.to_datetime(input_data['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "input_data['tpep_dropoff_datetime'] = pd.to_datetime(input_data['tpep_dropoff_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "input_data['PULocationID'].fillna(-1, inplace=True)\n",
    "input_data['DOLocationID'].fillna(-1, inplace=True)\n",
    "input_data['tpep_pickup_month'] = input_data['tpep_pickup_datetime'].dt.month\n",
    "input_data['tpep_dropoff_month'] = input_data['tpep_dropoff_datetime'].dt.month\n",
    "input_data['tpep_pickup_day_numeric'] = input_data['tpep_pickup_datetime'].dt.day\n",
    "input_data['tpep_dropoff_day_numeric'] = input_data['tpep_dropoff_datetime'].dt.day\n",
    "input_data['tpep_pickup_day_name'] = input_data['tpep_pickup_datetime'].dt.day_name()\n",
    "input_data['tpep_dropoff_day_name'] = input_data['tpep_dropoff_datetime'].dt.day_name()\n",
    "input_data['tpep_pickup_hour'] = input_data['tpep_pickup_datetime'].dt.hour\n",
    "input_data['tpep_dropoff_hour'] = input_data['tpep_dropoff_datetime'].dt.hour\n",
    "input_data['tpep_pickup_day'] = input_data['tpep_pickup_datetime'].dt.strftime(\"%w\").astype(int)\n",
    "input_data['tpep_dropoff_day'] = input_data['tpep_dropoff_datetime'].dt.strftime(\"%w\").astype(int)\n",
    "input_data['tpep_pickup_datetime'] = input_data['tpep_pickup_datetime'].apply(lambda x: int(x.strftime('%Y%m%d')))\n",
    "input_data['tpep_dropoff_datetime'] = input_data['tpep_dropoff_datetime'].apply(lambda x: int(x.strftime('%Y%m%d')))\n",
    "input_data = input_data.fillna(0)\n",
    "input_data = input_data.drop([\"tpep_dropoff_day_name\"], axis=1)\n",
    "input_data = input_data.drop([\"tpep_pickup_day_name\"], axis=1)\n",
    "input_data = input_data.drop([\"store_and_fwd_flag\"], axis=1)\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalisierung der Daten\n",
    "In diesem Abschnitt werden die Traingsdaten normalisiert. Im Anschluss werden diese in einen Trainings- sowie Testdatensatz aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Definition der Features und der TargetVariable (y)\n",
    "Features = ['PULocationID', 'DOLocationID', 'tpep_pickup_month', 'tpep_pickup_day', 'tpep_pickup_hour']\n",
    "TargetVariable = ['total_amount']\n",
    "\n",
    "#laden der Daten in einen seperaten Dataframe\n",
    "X = input_data[Features].values\n",
    "y = input_data[TargetVariable].values\n",
    "\n",
    "# definiert StandardScaler für das normalisieren Daten\n",
    "PredictorScaler = StandardScaler()\n",
    "TargetVarScaler = StandardScaler()\n",
    "\n",
    "# identifiziert das minimum und maximum Daten für die spätere Transformation\n",
    "PredictorScalerFit = PredictorScaler.fit(X)\n",
    "TargetVarScalerFit = TargetVarScaler.fit(y)\n",
    "\n",
    "# transformiert die Daten (Normalisierung)\n",
    "X = PredictorScalerFit.transform(X)\n",
    "y = TargetVarScalerFit.transform(y)\n",
    "\n",
    "#teilt die Daten in Test- und Traingsdaten auf (30% Testdaten, 70% Trainingsdaten)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelldefintion für das Regressionsmodell\n",
    "\n",
    "Modell zur Vorhersage von Fahrtkosten einzelnen Fahrten. Dafür wird nur eine Output-unit verwendet. Des Weiteren wird der mean squared error als loss-function verwendet, da es sich bei dem Modell um ein Regressionsmodell handelt. Die Parameter wurden durch testen identifiziert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=32, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(units=1)\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.fit(x=X_train, y=y_train, batch_size=100, epochs=20, shuffle=True,\n",
    "          verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Vorhersage mit dem Testdaten\n",
    "Predictions = model.predict(X_test)\n",
    "\n",
    "# die inversive Transformation der Vorhersagen, labeldaten und Testdaten\n",
    "Predictions = TargetVarScalerFit.inverse_transform(Predictions)\n",
    "y_test_orig = TargetVarScalerFit.inverse_transform(y_test)\n",
    "Test_Data = PredictorScalerFit.inverse_transform(X_test)\n",
    "\n",
    "#Erstellen eines neuen Dataframes mit den vorhergesagten Fahrtzeiten und den tatsächlichen Fahrtzeiten\n",
    "TestingData = pd.DataFrame(data=Test_Data, columns=Features)\n",
    "TestingData['total_amount'] = y_test_orig\n",
    "TestingData['predicted amount'] = Predictions\n",
    "TestingData.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zweiter Testdatensatz: Vorhersage für Fahrtkosten aus dem Jahr 2020 (Mai)\n",
    "In dem diesem Abschnitt werden nun Testdaten aus dem Jahr 2020 verwendet. Für diese Testdaten soll das vorher erstellte Model nun Vorhersagen erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Im ersten Schritt wird die csv Datei geladen. Daraufhin wird ein identisches Preprocessing der Daten durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Einlesen einer weiteren csv aus dem nächsten Jahr\n",
    "input_test_data = pd.read_csv(\"../data/processed/yellow_tripdata_2020-05.csv\", nrows=50)\n",
    "\n",
    "#Data preprocesssing\n",
    "input_test_data['tpep_pickup_datetime'] = pd.to_datetime(input_test_data['tpep_pickup_datetime'],\n",
    "                                                         format='%Y-%m-%d %H:%M:%S')\n",
    "input_test_data['tpep_dropoff_datetime'] = pd.to_datetime(input_test_data['tpep_dropoff_datetime'],\n",
    "                                                          format='%Y-%m-%d %H:%M:%S')\n",
    "input_test_data['trip_duration'] = (\n",
    "        input_test_data['tpep_dropoff_datetime'] - input_test_data['tpep_pickup_datetime']).dt.seconds\n",
    "input_test_data['trip_times (min)'] = input_test_data['trip_duration'].apply(lambda x: x / 60)\n",
    "input_test_data['PULocationID'].fillna(-1, inplace=True)\n",
    "input_test_data['DOLocationID'].fillna(-1, inplace=True)\n",
    "input_test_data['tpep_pickup_month'] = input_test_data['tpep_pickup_datetime'].dt.month\n",
    "input_test_data['tpep_dropoff_month'] = input_test_data['tpep_dropoff_datetime'].dt.month\n",
    "input_test_data['tpep_pickup_day_numeric'] = input_test_data['tpep_pickup_datetime'].dt.day\n",
    "input_test_data['tpep_dropoff_day_numeric'] = input_test_data['tpep_dropoff_datetime'].dt.day\n",
    "input_test_data['tpep_pickup_day_name'] = input_test_data['tpep_pickup_datetime'].dt.day_name()\n",
    "input_test_data['tpep_dropoff_day_name'] = input_test_data['tpep_dropoff_datetime'].dt.day_name()\n",
    "input_test_data['tpep_pickup_hour'] = input_test_data['tpep_pickup_datetime'].dt.hour\n",
    "input_test_data['tpep_dropoff_hour'] = input_test_data['tpep_dropoff_datetime'].dt.hour\n",
    "input_test_data['tpep_pickup_day'] = input_test_data['tpep_pickup_datetime'].dt.strftime(\"%w\").astype(int)\n",
    "input_test_data['tpep_dropoff_day'] = input_test_data['tpep_dropoff_datetime'].dt.strftime(\"%w\").astype(int)\n",
    "input_test_data['tpep_pickup_datetime'] = input_test_data['tpep_pickup_datetime'].apply(\n",
    "    lambda x: int(x.strftime('%Y%m%d')))\n",
    "input_test_data['tpep_dropoff_datetime'] = input_test_data['tpep_dropoff_datetime'].apply(\n",
    "    lambda x: int(x.strftime('%Y%m%d')))\n",
    "input_test_data = input_test_data.fillna(0)\n",
    "input_test_data = input_test_data.drop([\"tpep_dropoff_day_name\"], axis=1)\n",
    "input_test_data = input_test_data.drop([\"tpep_pickup_day_name\"], axis=1)\n",
    "input_test_data = input_test_data.drop([\"store_and_fwd_flag\"], axis=1)\n",
    "input_test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalisierung der Daten\n",
    "In diesem Schritt wird analog zu der vorherigen Normalisierung vorgegenagen. Nur in diesem Fall ist es nicht notwenig die labels seperat zu transformieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#laden der Daten in einen seperaten Dataframe\n",
    "X_test_features = input_test_data[Features].values\n",
    "y_test_target = input_test_data[TargetVariable].values\n",
    "\n",
    "# definiert StandardScaler für das normalisieren Daten\n",
    "test_PredictorScaler = StandardScaler()\n",
    "test_TargetVarScaler = StandardScaler()\n",
    "\n",
    "# identifiziert das minimum und maximum Daten für die spätere Transformation\n",
    "test_TargetVarScalerFit = test_TargetVarScaler.fit(y_test_target)\n",
    "test_PredictorScaler = test_PredictorScaler.fit(X_test_features)\n",
    "\n",
    "# Transformation der Testdaten\n",
    "X_test_features = test_PredictorScaler.transform(X_test_features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vorhersage mit den Testdaten\n",
    "second_predictions = model.predict(X_test_features)\n",
    "\n",
    "# die inversive Transformation der Vorhersagen und Testdaten\n",
    "second_predictions = test_TargetVarScaler.inverse_transform(second_predictions)\n",
    "second_Test_Data = test_PredictorScaler.inverse_transform(X_test_features)\n",
    "\n",
    "#Erstellen eines neuen Dataframes mit den vorhergesagten Fahrtzeiten und den tatsächlichen Fahrtzeiten\n",
    "second_TestingData = pd.DataFrame(data=second_Test_Data, columns=Features)\n",
    "second_TestingData['total_amount'] = y_test_target\n",
    "second_TestingData['predicted amount'] = second_predictions\n",
    "second_TestingData.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualisierung der Ergebnisse\n",
    "Nun\n",
    "werden\n",
    "die\n",
    "vorhergesagten\n",
    "Fahrtzeiten\n",
    "mit\n",
    "den\n",
    "tatsächlichen\n",
    "Fahrtzeiten\n",
    "gegenüber\n",
    "gestellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(second_TestingData['total_amount'], color='red', label='Echter Betrag')\n",
    "plt.plot(second_TestingData['predicted amount'], color='blue', label='Vorhergesagter Betrag')\n",
    "plt.title('Gegenüberstellung der vorhergesagten und tatsächlichen Fahrkosten')\n",
    "plt.xlabel('Fahrtnummer')\n",
    "plt.ylabel('Betrag (in $)')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(1000, 1000))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In dem folgenden Diagramm wird die Regressionsergebnisse in einem Scatter dargestellt."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#scatter für die Darstellung der Ergebnisse\n",
    "sn.regplot(x='total_amount', y='predicted amount', data=second_TestingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}